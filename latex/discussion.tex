% ETL state
The choice of ELT without a dedicated ETL/ELT software has several advantages.
It factors both people's knowledge and computer resources allowing 
analysts to become implementers and revise code or contribute to transformation 
with SQL as the unique language and technology.

% Organisation of the code
By choosing a public git repository for documentation and source code support, 
this allows analysts to learn more about the project and learn how to 
contribute \cite{mimic-git}.

% Choice of ELT

% Data quality
Any data transformation is likely to generate bugs that can have a huge impact 
in medical research. The foundations of the Relational database management system (RDBMS)
, such as transactions, standardization and integrity constraints, are integrated 
safeguards that have been useful throughout the process. In addition to the implemented, 
unit tests ensure that past or future bugs are behind us. An ideal but complex 
validation method \cite{johnson-reprod} would be to replicate existing OMOP 
studies and ensure that the results are consistent. 


% ELT Time
The calculation time of the ETL on the PgSQL instance on a modest personal 
computer is compatible with a community work where the collaborator can clone 
the source code and configure a development instance to reproduce or improve 
the work. The choice of ELT based mainly on SQL code allows end users with 
SQL background only to review and improve the work. 
As a result, the target community is as broad as possible and we expect 
translation profiles to be involved.

% SQL: a choice for future
The datathon showed that platforms distributed with basic hardware provide SQL 
tools for Online Analytical Processing  with excellent performance that overcome 
OnLine Transaction Processing RDBMS weaknesses. 
Therefore, it takes advantage of SQL language analysis functions 
such as grouping, windowing, assembling and mathematical functions that are 
often missing in NoSQL databases.

% about derived tables
It is important that OMOP maintains a level of standardisation in order to 
simplify ETL and make it consistent. However, once done, it makes sense to give 
access to scientific data at more denormalized and special tables. 
There are many concerns about OMOP's performance and optimization. However, 
there will never be a perfect multi-purpose case table, and it is the 
responsibility of the data scientist to build his own, simplified, specialized 
tables for his research and to respond effectively and clearly to his needs.


% About derived data
The derived data integrate quite well into OMOP. We used \textit{note\_nlp} to
store information derived from scores, \textit{measurement} to store numerical
information and \textit{cohort\_attribute} to store scores. However, it is not
yet clear whether derived data should be stored by domain or whether it should
be stored in dedicated derived tables. We found that there are no tables to
track the source and description of these data. 


% missing in the model
%% data quality
Another missing aspect is the quality tables for assessing and measuring data 
quality. MIMIC had a column to keep track of corrupted information. 
It would be interesting to be able to keep the disordered data and allow 
research on data cleaning and quality and avoid deleting data in training.

%% mecanisms for real time ingestion of data, such version control
Last but not least, as noted in the introduction, a good CDM for the ICU would 
allow for near real-time early warning systems and inference modelling on fresh 
data. OMOP is clearly designed to provide a static data set and does not have 
real-time ingestion and data version control mechanisms - it is not a data 
warehouse. Analysis of static data sets is essential for reproducible results. 
However, when the algorithm needs to be moved to the bed side, it is necessary 
to have fresh data and a means of identifying the patient that OMOP will not 
easily provide. That said, a solution like FHIR is a great way to implement 
real-time inference from EHR data, and that's how FHIR and OMOP are complementary. 
This has already been studied \cite{gatech}  but needs further optimisation.

% OMOP community organisation
During this work, the OMOP forum was very active. It is a challenge to manage 
such a large community of all moderators, contributors and from the user's 
point of view. It seems that it is not possible for most people to get involved. 
The forum is full of details and in training. It contrasts with the implementation 
guide, which suffers from not being as detailed. We believe that the OMOP 
community would greatly benefit from a systematic and synthetic synchronization 
between the forum, mailing lists, github and end user documentation.

% help from datathon: specialized mapping

% Documentation need
The real life test of the datathon revealed the strong need to make the physical 
data model accessible, including comments on columns and tables, and we 
discovered that the open-source tool schema spy tool was a good help. 
In addition, we found that the git repository is the best place to document 
and interact with the community.

% data / algorithm sharing
The conversion of MIMIC to OMOP format is finally the first step in our plan. 
In France, the exchange of patient data between centres is rightly very regulated. 
We think that MIMIC-OMOP could be used to learn OMOP and build algorithms. 
Then it is necessary to send the data we imagine to send the algorithms in the 
centers having transcribed their database in OMOP format. 
This would resolve confidentiality issues.
