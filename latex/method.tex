%
% Data Transformation
%
\subsection{Data Transformation}

%
% General
% (version, people, tools, ELT)

% github
All \textbf{transformation processes} are freely accessible to the public via
the github website \cite{mimic-omop-website} maintained by MIT-LCP
\cite{mimic-nature}. The repository is based on git and is designed for
sharing, improvement, collaboration and reproducible work. Indeed, github is
archived on a universal and durable software archive solution
\cite{universal-archive}. The github repository centralizes the various
resources of this work such as documentation, source code, unit tests, as well
as questioning examples, discussions and problem issues.  It also indicates web
resources such as the physical data model for MIMIC\cite{mimic-schemaspy} and
OMOP\cite{omop-schemaspy} datasets and the Achilles' web
client\cite{mimic-omop-achilles}.
% tools
The vast majority of source code is implemented in PostgreSQL 9.6.9 (PgSQL) 
because it is the primary support for the MIMIC database and allows the community 
to run our work on limited resources without needing a license. 
Finally, PgSQL has recently made huge efforts to better manage data processing. 
Some elaborate data transformations have been implemented as PgSQL functions.
% data version
MIMIC-III version 1.4.21 (MIMIC) has been loaded into pgSQL with the provided
scripts. The OMOP CDM version 5.3.3.1 (OMOP) target tables were created from
the provided scripts with some small changes stored in the change script. OMOP
which defines 15 standardized \textit{clinical} data tables, 3 \textit{health}
system data tables, 2 \textit{health economics} data tables, 5 tables for
\textit{derived} elements and 12 tables for standardized \textit{vocabulary}.
From MIMIC, we only used the clinical and derived tables. 


\\

% Structural Mapping 
% plan:
% evaluation : dispatching, domains, relationships, statistics
% loss of information, modification of the structure
The \textbf{structural mapping} aims at moving the MIMIC data into the right
place in OMOP with some data transformations. It consists of three
phases: conception, implementation and evaluation.

% conception step
The conception phase has been looping over each MIMIC table and choosing an
equivalent location in OMOP for each column. In general, the MIMIC
documentation and the OMOP documentation were sufficiently informative. In
several cases, we needed clarification from MIMIC contributors on the dedicated
github repository, or from the OMOP community forum. Some complicated choices
have been discussed in the repository \cite{mimic-omop-github} and can be
tracked in the commit log.

% implementation step
The implementation is generally done by an Extract-Transformation-Load (ELT)
process which is a methodology for migrating data from a source to a target
location. ETL first extracts the data from the source location, then applies
the transformations to a dedicated computer and finally loads the resulting
data into the target location. Extract-Load-Transform (ELT) is a slightly
different methodologies that does not use a dedicated server transformation.
The data is extracted and loaded directly into the target location and
subsequently transformed into the location. The ELT is composed of PgSQL
scripts, each extracting information from the source or concept mapping tables,
then transforming and loading an OMOP target table. The order of these scripts
is important and is done sequentially through a main script.
% speeding up (subset, indexes, and unlogged tables)
Measures have been taken to allow fast development cycles. A subset of 100
patients over the 46K total MIMIC patients was selected based on their broad
representativeness of the database and cloned into a second instance to serve
as a light and representative development set. OMOP indexes that would have
slowed data migration with unnecessary calculations have been removed.
% modifications of the OMOP structure
Finally in last resort some modification of the structural model of OMOP have
been made. A dedicated script recaps all of them. It contains columns name
modifications, new columns, columns type modifications or database indexing
modification.

% evaluation step 
The evaluation step is a set of controls to guaranty a correct transformation.
Each ELT script has been tested using pgTAP, a unit test framework for PgSQL.
This enable checking for loss of information, or regression during development.
Each unit test script checks whether a particular OMOP target table is loaded
correctly - most tables are covered and tests cover simple counts, aggregate
counts or distribution checks.
% statistics evaluation of loss
In order to compare overall statistics, some SQL queries have been setup to
compare MIMIC and MIMIC/OMOP. The resulting tables are presented in the results
section. Integrity constraints (primary keys, foreign keys, non-nullable
columns) have been included to apply integrity checks at runtime. 
% the sequence
In particular, a global unique sequence have been introduced to identify
relations between tables. Each source table has been added a global unique
sequence incremented from 0 that serves as the primary key and links in the
OMOP target tables. Each record in MIMIC is uniquely identified allowing to
chain the information with OMOP while simplifying the primary/foreign key
maintenance.
\\


% Conceptual Mapping 
The \textbf{conceptual mapping} aims at aligning the MIMIC local terminologies
to OMOP standard ones. It consists of three phases: integration, alignment and
evaluation.

The integration phase is about loading both kind of terminologies into the OMOP
vocabulary tables.
% Athena codes
The OMOP terminologies are provided by the Athena tool \cite{ohdsi-athena} and
where loaded with the provided programs. We have used an export with all
terminologies without licensing limitations.
% MIMIC local codes
The local terminologies have been extracted from the multiple MIMIC tables and
loaded in the unique OMOP concept table. When possible, relevant information
from the original MIMIC tables has been concatenated in the
\textit{concept\_name} column. 
New local MIMIC concepts were introduced and given a value from 2 billion to
distinguish them from local MIMIC concepts.  MIMIC local codes are also loaded
into the vocabulary table with a \textit{concept\_id} identifier from 2.1
billion (below this number is reserved for OMOP terminologies
\cite{omop-documentation-pdf}). 
MIMIC codes can also be distinguished with \textit{the vocabulary\_id}
identifier equal to "MIMIC code" and a \textit{domain\_id} identifier targeting
the OMOP table in which the corresponding data is stored. Later, this domain
information is sometime used in the ELT to send the information in the proper
table with a so called "concept-driven dispatching".

%alignment phase
The alignment phase to standardizing local MIMIC codes into OMOP standards
codes, had four distinct cases. In the \emph{first} case, MIMIC is by chance
already in OMOP standard terminology (e.g. LOINC laboratory results) and,
therefore, the standard and local concepts are the same. In the \emph{second}
case, MIMIC is not in the standard OMOP terminology, but the mapping is already
provided by OMOP (ex: ICD9/SNOMED-CT), so the domain tables have been loaded
accordingly.  In the \emph{third} case, mapping is not provided, but it is
small enough to be done manually in a few hours (such as demographic status,
signs and symptoms). In the \emph{fourth} case, mapping is not provided and
terminology is enormous (admission diagnosis, drugs). Then, only a subset of
the most represented code was manually mapped.
% fuzzy match
When the mapping concept is required manually, a mapping csv file has been
built.  This solution can be adapted to medical users who do not have training
in database engineering. The spreadsheet has several columns such as
local/standard labels, ids and also comments, evaluation metrics and a script
loads them into the PgSQL when completed. In order to catalyse the mapping
process, the language algorithm has proven to be effective
\cite{schema-matching} although OHDSI provides USAGI \cite{usagi}. We have
chosen to use simple SQL queries that are flexible enough to be queried on
demand or to generate a pre-filled csv with the best matches. It uses PGSQL
full-text ranking features  and links local and standard candidates with a
rating function based on their labels. This work was followed by a intensivist
check.

% evaluation
The evaluation phase was of both qualitative and quantitative nature.
The qualitative evaluation for newly generated mapping has consisted of tagging
each mapping with a score between 0 and 1 and eventually write a commentary on
each mapped concept. In case the mapping was provided by OMOP the evaluation
was made manually by picking some concepts of each terminology.
The quantitative evaluation measures the percent of concepts that are mapped to
a standard with a SQL query.

%
% Contributions
%
\subsection{Contribution}

% scores
MIMIC provides a large number of SQL scripts to calculate derived scores and
defined cohorts as known as "contrib". Some of them have been implemented on
top of the OMOP format to load the OMOP derived tables. 

% denormalized tables
A set of \emph{general denormalized} tables has been built on top of the original 
OMOP  format that have the \textit{concept\_name} related to the \textit{concept\_id} 
columns. The concept table is a central element of OMOP and, therefore, it is 
involved in many joins to obtain the concept label. Normalized tables accelerate 
calculation time and provide an easier set of data for analysis.

% specialized materialized views
In addition, a set of \emph{specialized materialized analysis views} has been built 
on the original OMOP format. Microbiologicalevents table is a reorganization of the 
measurement table data of microorganisms and associated susceptibility testing 
antibiotics and is based on the MIMIC \textit{microbiologicalevents} table. 
The OMOP icustays table allows to quickly obtain the patients admitted in 
resuscitation and is inspired by the MIMIC \textit{icustays} tables.

% note nlp
The \textit{note\_nlp} table was originally designed to store final or intermediate 
derived information and metadata from clinical notes. When definitive, the 
extracted information is intended to be moved to the dedicated domain or table 
and then reused as regular structured data. 
When the information is still intermediate, it is stored in the \textit{note\_nlp} 
table and can be used for later analysis. 
To assess this table, we provided two information extraction pipelines. 
The \emph{first} pipeline extracted numerical values such as weight, height, 
body mass index and left ventricular cardiac ejection fraction from medical notes 
with a SQL script. The resulting structured numerical values were loaded into the 
measurement or observation tables according to its domain. 
The \emph{second} pipeline \emph{section extractor} based on the apache UIMA 
framework divides notes into sections to help analysts choose or avoid certain 
sections of their analysis. While some methods already exist to extract 
medical sections \cite{section-extraction}, the prior work of describing sections 
was too high, and we opted for a naive approach. Section templates (such as 
"Illness History") have been automatically extracted from text with regular 
expressions, then filtered to keep only the most frequent (frequency > to 1\%). 

%
% Analytics
%
\subsection{Data Analytics}

% datathon
A 48-hour open access datathon \cite{mimic-omop-datathon} was set up in Paris AP-HP 
(Assistance Publique des Hopitaux de Paris) once the MIMIC-OMOP transformation was 
ready for research to evaluate OMOP as an alternative data model in a real event. 
This datathon was organised in collaboration with the MIT.
Scientific questions had been prepared in an online forum. Participants could 
introduce themselves and propose a topic or choose an existing one. 
OMOP has been loaded into apache HIVE 1.2.1 in ORC format. Users had access to 
the ORC dataset from a web interface jupyter notebooks with python, R or scala. 
A SQL web client allowed teams to write SQL from presto to the same dataset. 
The hadoop cluster was based on 5 computers with 16 cores and 220GB of RAM memory. 
The MIMIC-OMOP dataset has been loaded from a PGSQL instance to HIVE thought apache 
SQOOP 1.4.6 directly in ORC format. Participants also had access to the Schemaspy 
database physical model to access the OMOP physical data model with both 
table/column comments and key primary/foreign relationships materializing the 
relationships between the tables. All queries have been logged.

