\subsection{Data Transformation}

Ajouter les deux schemas de database design

- To provide standard and reproductilable process all the ETL used SQL script
  with PostgreSQL-ETL implementation
- we tried  not to infer results. For examens whereas it's logical to put a
  specimen for many labevents results (as one sample of blood may be used to
  multilple exams) we decided to create as many specimen row as laboratory
  exams because the information is not present in MIMIC. It was the same when
  date information were not provide (start/end_datetime for drug_exposure)

% ETL
The MIMIC III version 1.4 21 clinical data tables and 5 tables for vocabulary
- OMOP CDM v5.3.1 which defines 15 standardized \it{clinical} data tables, 3
\it{health} system data tables, 2 \it{health economics} data tables, 5 tables
for \it{derived} elements and 12 tables for standardized \it{vocabulary}. 
We didnâ€™t use the health economics data tables (not provided by MIMIC)

All the process is available freely on the github website :
https://github.com/MIT-LCP/mimic-omop.

Extract-Transformation-Load (ETL) processes is a methodology that allows
migrating data from a source location to a target location. ETL first extract
the data from the source location, and then apply transformations such
structural modifications or conceptual modifications generally on a dedicated
computer and technology. The last step is to load the resulting data into the
target location. Transformation program are generally written in a programming
language such java, c++ or python.
Extract-Load-Transform (ELT) processes is a slightly different methodology that
does not use a dedicated transformation server and limits data transfers. The
data is extracted and directly loaded into the target location. It is
transformed afterwards in place. ELT allows to factorize both computer
resources, and people knowledges. Indeed transformations are then written in a
database dialect such SQL, as well as source and target location. Improving
database resources will then benefit for both ETLers and end users. Since RDBMS
computer resources cannot scale well and does not provide a good support for
procedural language, ETL have been for long time used in conjonction with
RDBMS. The emergence of distributed platform such hadoop allows to take part
of ELT because they both allow to scale well horizontally and write java
procedural user defined function that are used in conjonction with SQL queries.
In this work, we decided to transform MIMIC into OMOP thought a ELT to limit
the programming knowledges needed for code maintenance and to allow end users
to participate in this process. PostgreSQL has been chosen as the database
support for ELT because it is the primary support of MIMIC database and allows
community to run the ETL on limited resources without licensing need. Finally
PostgreSQL have recently made huge effort to handle data-processing better.

% Structural Transformations

% creating tables (only with constraints, no indexes, subset, sequences)
The MIMIC source data have been loaded with the provided scripts into a
PostgreSQL (PGSQL) instance and a subset of 100 patient selectioned upon their
broad representativity in terms of data have been cloned into a second instance
to serve as a light, representative development set.
Each tables of the source have been added a global sequence incremented from 0
that would serve as primary key and link into the OMOP target tables. The
target tables have been created from the provided script exept the indexes
that would have slow down the data migration with useless computations. The
integrity constraints (primary keys, foreign keys, non nullable columns) have
been included to apply sanity checks at runtime. Some tricky transformations
have been implemented as PGsql functions.
% unit testing
Each ETL part have been tested thought PGtap, a unit testing framework for PGSQL.
This allows to make sure there is no loss of informations, or code regression
during the ETL development or for further updates of the source code.
% DDL modification
A SQL script updates the OMOP tables when needed (the list of the modifications
is detailed bellow). 
All character typed columns limited in length have been changed to unlimited
since this might cause inpredictable truncation of content, and this has no bad
impact on PGSQL's storing size or performance.
The \it{visit\_occurrence} and \it{visit\_detail} table have been corrected
accordingly to some discussion on the forum.
The \it{note\_nlp} table have been extended with some fields accordingly to
the documentation online. The character \it{offset} column have been split into
two integers columns because the offset word is a SQL reserved word and it
makes sense to fill the resulting \it{offset\_begin} and \it{offset\_end}
resulting columns.

The structural transformation have been done in few iteration of several phases.
The first phase consists of looping over each MIMIC table and choose for each
columns an equivalent location in OMOP. In general, the MIMIC documentation and
the OMOP documentation were sufficient informative. In several cases, we needed
to get clarification from the MIMIC contributors on the dedicated github
repository, or from the OMOP community on the dedicated forum.
All choices have been discussed in the MIMIC-OMOP github issues, and can be
tracked into the commit log. The resulting table to table workflow is
materialized in the repository as bi-directional documentation: from MIMIC
table to OMOP table and vice-versa.

[DISCUSSION?] During this work the OMOP forum was very active. Working groups.
It is a challenge to manage such large community from all moderator,
contributors and from a user perspective. It appears it is not doable for most
of people to get involved. The forum is full of details and information. It
contrast with the implementation guide that suffer from not being as well
detailed. We think the OMOP community would greatly benefit from systematic
and synthetic synchronisation between forum, github and end user documentation.

1.3 Preprocessing and modification of mimic
+++++++++++++++++++++++++++++++++++++++++++

- We added emergency stays as a normal location for patients throughout their
  hospital stay.
- Icustays mimic table was deleted as it is a derived table from transfers
  table (2) and we decided to assigne a new new visit_detail pour each stay in
  ICU (based of the transfers table) whereas mimic prefered to assigne new
  icustay stay if a new admissions occurs > 24h after the end of the previous
  stay


- modification of MIMIC
	- visit_detail : admitting_source_value, admitting_source_concept_id, admitting_concept_id, discharge_to_source_value, discharge_to_source_concept_id, discharge_to_concept_id provide redondant information from visit_occurrence. We did't populate it.
	- observation_period provide duplicate informations with visit_occurrence : we fill this table to respect the omop model and tools
	- operators have been extracted to fill operator_concept_id
	- units of measures have been extracted to fill unit_concept_id
	- numeric values have been extracted to fill value_as_number

% Conceptual Transformations

- concept-driven methodology : as the omop model did we adopt a "concept-driven
  methodology", domain of each local concept drive the concept to the right
  table.

	- conceptual (new concepts specific to ICU or general)
		- measurement_type_concept_id
		- the actual visit_detail doesn't introduce pertinent information and duplicate informations from visit_occurrence table. For admitting_from_concept_id and discharge_to_concept_id, we extended the dictionary in order to track bed transfers and ward transfers. For visit_type_concept_id we assigned a new concept for any level of granularity necessary for your use case (ward, bed...) 
		<!-- Fournir un example de visit_detail-->

% local coding
The conceptual transformation consists on both loading OMOP with the MIMIC
local concepts, and mapping them to the standards concepts. 
The local codes have been loaded in the concept table starting from 2 billion
as specifided in the OMOP documentation \cite{omop-documentation-pdf}. The
MIMIC local codes are then added to the OMOP provided code (starting from 1 to
2 billion) and can be distinguished with the domain_id equals to MIMIC code. As
much as possible information from the MIMIC coding table have been loaded in
order to allow analytics with the local code without loosing information as
compared to the MIMIC original model (worst case we concatened information as
free text in the concept_name column).
%concept mapping
To deal with concept mapping for each domain a csv file have been built as a
support for manual standard mapping. This solution can scale for medical users
without database engineering background. We tried to adopt the same methodology
in their creation ; some obvious fields are needed : local and standard name,
local and standard id. Moreover evaluation and comments fields are good
practices and may help contributers Those csv file are automatically loaded
into PGSQL and get the sequence added too. There were four distinct cases.  In the
\emph{first} case MIMIC is already in the OMOP standard terminology (eg: LOINC
laboratory results), the work is already made.  In the \emph{second} case the
mapping is already provided by OMOP (eg: ICD9/SNOMED-CT) then the data tables
have been loaded accordingly.  In the \emph{third} case the mapping is not
provided, but is small enough to be done manually in few hours (such as
demographic status, signs and symptoms).  In the \emph{fourth} case the mapping
is not provided and the terminology is huge (such admission diagnoses, drugs).
Then we decided to only map the subset of the code that are the most
represented.
% fuzzy match
In all case a mapping has to be done we have setup a semi-automatic methodology
to make auto-suggestion. Many mapping tools exist on the area RELMA provided by
LOINC, USAGI provided by OHDSI. Most of those tools are based on linguistic
mapping [cite], and the approach have been shown to be the most
effective[cite]. Following our prime idea to build low dependency tools, we
managed to build a light semi-automatic tool based on PGSQL full-text ranking
feature.  Once the concept table has been loaded with both standard and local
concepts the full text index ranks the top n standard concepts that best match
the local codes based on description or label.  This work was followed and
check by a physician.

- The key table for omop is the concept table. The standard vocabulary of OMOP
is mainly based on the Systematized Nomenclature of Medicine Clinical Terms
(SNOMED-CT)
- A mapping between many classifications and the standard omop ones (ICD-9 and
snomed-CT for examples) is already provided with concept_relationship table. We
have used this to the maximum extent possible (laboratory exams, exit diagnoses
and procedures)

%Head 2
\subsection{Data Analytics}
% datathon
- datathon / organization
A total of 150 person and 20 teams from X countries were present for the two
days event. 20 projects had been prepared thought a forum.
- technical architechure
OMOP have been loaded into apache HIVE 1.2.1. into ORC format. Users had access
to the ORC dataset from jupyter notebooks with, python or scala. A SQL
webclient allowed teams to write SQL from presto on the same dataset. The
hadoop cluster was a based on 5 computers with 16 cores and 220GO ram.
The MIMIC-OMOP dataset has been loaded from a PostgreSQL instance to HIVE
thought apache SQOOP 1.4.6. directly into the ORC format. 
Participants had also access to the physical modeling of the database thought
schemaspy to have access to both table/column comments and primary/foreign key
materialising tables relations.
\subsection{Contribution}
% scores
MIMIC provides a lot of SQL scripts to calculate derived scores and existing
cohorts. Some of them have been translated based on the OMOP data and
populates the OMOP cohort tables.
%TODO: talk about translation of MIMIC views
Unprecedented derived informations have been introduced and loaded such
corrected calcemia, kaliemia, P/F ratio, corrected osmolarity

% denormalized tables
A set of \emph{general denormalized} tables has been built on top of the
original OMOP format wich have the concept\_name from the concept table for
both standard and local codes. The concept table is a central piece of the OMOP
format and as a result it is involved in many joins to get the concept label. 
% specialized tables 
A set of \emph{specialized analytics} tables has been built on top of the
original OMOP format. The microbiology events is a reorganization of data from
measurements for microorganisms and related antibiograms and is inspired from
MIMIC \it{microbiology_event} table. 
% note nlp
The \it{note_nlp} table allows to store Natural Language Processing results
derived from plain text notes. In order to evaluate this table we provided
two information extraction pipelines.
The \emph{first} pipeline \emph{section extractor} based on apache UIMA framework
splits the notes into sections in order to help analysts to choose or avoid
some sections from their analysis.  The sections patterns (such "Illness
History") have been automatically extracted from texts from regular
expressions, automatically filtered by keeping only one with frequency higher
than 1 percent and manually filtered to exclude false positives with a total of
1200 sections. The resulting sections patterns candidate have been then
manually regrouped into similar 400 groups.  The extracted sections have not
been mapped to the any standard terminology such LOINC CDO. The reason is the
CDO LOINC has decided to stop to maintain and to remove it's sections from its
standard arguing it is too difficult to maintain, and this sections are not
widely used
[https://loinc.org/news/loinc-version-2-63-and-relma-version-6-22-are-now-available/].
The \emph{second} pipeline has extracted some numeric values such weight,
height, body mass index and cadriac left ventricular ejection fraction within
medical notes with a python script. The resulting structured numerical values
have been loaded in the measurement or the observation tables.
